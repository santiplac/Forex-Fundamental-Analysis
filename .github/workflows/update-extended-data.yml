name: Update Extended Data (Bonds + Sentiment + Flows) - FINAL
on:
  schedule:
    - cron: '30 6 * * *'  # Daily at 6:30 AM UTC
  workflow_dispatch:

jobs:
  update-extended-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests beautifulsoup4 lxml --break-system-packages

      - name: Create directory
        run: mkdir -p extended-data

      - name: Scrape extended indicators
        run: |
          python3 << 'EOFPYTHON'
          import requests
          from bs4 import BeautifulSoup
          import json, re, os, time
          from datetime import date, datetime

          HEADERS = {
              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
              'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
          }

          CURRENCIES = ['USD', 'EUR', 'GBP', 'JPY', 'CAD', 'AUD', 'CHF', 'NZD']

          COUNTRY_NAMES = {
              'USD': ['United States'],
              'EUR': ['Euro Area'],
              'GBP': ['United Kingdom'],
              'JPY': ['Japan'],
              'CAD': ['Canada'],
              'AUD': ['Australia'],
              'CHF': ['Switzerland'],
              'NZD': ['New Zealand']
          }

          TE_URLS = {
              'bond10y':               'https://tradingeconomics.com/country-list/government-bond-yield',
              'consumerConfidence':    'https://tradingeconomics.com/country-list/consumer-confidence',
              'businessConfidence':    'https://tradingeconomics.com/country-list/business-confidence',
              'capitalFlows':          'https://tradingeconomics.com/country-list/capital-flows',
              'fdi':                   'https://tradingeconomics.com/country-list/foreign-direct-investment',
              'inflationExpectations': 'https://tradingeconomics.com/country-list/inflation-expectations',
          }

          def clean_num(text):
              if not text: return None
              text = str(text).strip().replace(',','').replace('%','')
              m = re.search(r'(-?\d+\.?\d*)', text)
              return float(m.group(1)) if m else None

          def parse_te_date(date_text):
              if not date_text: return str(date.today())
              date_text = date_text.strip()
              try:
                  if re.search(r'[A-Za-z]{3}\s*[/\s]\s*\d{2,4}', date_text):
                      m = re.search(r'([A-Za-z]{3})\s*[/\s]\s*(\d{2,4})', date_text)
                      if m:
                          yr = m.group(2)
                          if len(yr) == 2: yr = '20' + yr
                          dt = datetime.strptime(f"{m.group(1)} {yr}", '%b %Y')
                          return dt.strftime('%Y-%m-15')
                  if re.search(r'Q\d[/\s]\d{4}', date_text):
                      m = re.search(r'Q(\d)[/\s](\d{4})', date_text)
                      if m:
                          month = (int(m.group(1)) - 1) * 3 + 2
                          return f"{m.group(2)}-{month:02d}-15"
              except: pass
              return str(date.today())

          def normalize_confidence(value, currency, indicator):
              """
              Normaliza Ã­ndices de confianza a escala base-100 (100 = neutral).

              Escalas reales por paÃ­s e Ã­ndice:
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Currency â”‚ Ãndice                                   â”‚ Escala original â”‚ NormalizaciÃ³n                      â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ USD     â”‚ Conference Board / Michigan              â”‚ 0-150+          â”‚ Pass-through (ya usable)           â”‚
              â”‚ EUR     â”‚ European Commission (ambos)              â”‚ -100/+100       â”‚ +100 shift                         â”‚
              â”‚ GBP     â”‚ GfK Consumer / CBI Industrial (ambos)   â”‚ -100/+100       â”‚ +100 shift                         â”‚
              â”‚ JPY     â”‚ Cabinet Office (consumer)                â”‚ 35-50 range     â”‚ fÃ³rmula ((v-40)/10)*15+100         â”‚
              â”‚ JPY     â”‚ Tankan Large Mfg (business)              â”‚ -100/+100       â”‚ +100 shift  â† BUG CORREGIDO        â”‚
              â”‚ AUD     â”‚ Westpac/ANZ (consumer)                   â”‚ ~100 base       â”‚ Pass-through                       â”‚
              â”‚ AUD     â”‚ NAB Business Confidence (business)       â”‚ -100/+100       â”‚ +100 shift  â† BUG CORREGIDO        â”‚
              â”‚ CAD     â”‚ Conference Board Canada (consumer)       â”‚ ~100 base       â”‚ Pass-through                       â”‚
              â”‚ CAD     â”‚ Ivey PMI (business)                      â”‚ 0-100           â”‚ Pass-through                       â”‚
              â”‚ CHF     â”‚ SECO (consumer)                          â”‚ -150/+50 aprox  â”‚ +130 shift  â† BUG CORREGIDO        â”‚
              â”‚ CHF     â”‚ KOF Barometer (business)                 â”‚ ~100 base       â”‚ Pass-through                       â”‚
              â”‚ NZD     â”‚ ANZ-Roy Morgan (consumer)                â”‚ ~100 base       â”‚ Pass-through                       â”‚
              â”‚ NZD     â”‚ ANZ Business Outlook (business)          â”‚ -100/+100       â”‚ +100 shift  â† BUG CORREGIDO        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              """
              if value is None:
                  return None

              # â”€â”€ EUR: European Commission â†’ -100/+100, neutral = 0
              if currency == 'EUR' and indicator in ['consumerConfidence', 'businessConfidence']:
                  return round(value + 100, 2)

              # â”€â”€ GBP: GfK / CBI â†’ -100/+100, neutral = 0
              if currency == 'GBP' and indicator in ['consumerConfidence', 'businessConfidence']:
                  return round(value + 100, 2)

              # â”€â”€ JPY consumer: Cabinet Office index, typical range 35-50, neutral â‰ˆ 40
              if currency == 'JPY' and indicator == 'consumerConfidence':
                  normalized = ((value - 40) / 10) * 15 + 100
                  return round(normalized, 2)

              # â”€â”€ JPY business: Tankan Large Manufacturing Index â†’ -100/+100, neutral = 0
              if currency == 'JPY' and indicator == 'businessConfidence':
                  return round(value + 100, 2)

              # â”€â”€ AUD business: NAB Business Confidence â†’ -100/+100, neutral = 0
              if currency == 'AUD' and indicator == 'businessConfidence':
                  return round(value + 100, 2)

              # â”€â”€ CHF consumer: SECO â†’ scale roughly -150 to +50, neutral â‰ˆ -30
              # Shift so that -30 (historical long-run average) = 100
              if currency == 'CHF' and indicator == 'consumerConfidence':
                  return round(value + 130, 2)

              # â”€â”€ NZD business: ANZ Business Outlook â†’ -100/+100, neutral = 0
              if currency == 'NZD' and indicator == 'businessConfidence':
                  return round(value + 100, 2)

              # All others pass through unchanged (already on a ~100 base scale)
              return value

          def scrape_te_table(url, label):
              print(f"\n{'='*50}\nSCRAPING: {label}\n{'='*50}")
              data, dates = {}, {}
              try:
                  r = requests.get(url, headers=HEADERS, timeout=20)
                  if r.status_code == 429:
                      print(f"  âš ï¸ Rate limited, waiting 30s...")
                      time.sleep(30)
                      r = requests.get(url, headers=HEADERS, timeout=20)
                  r.raise_for_status()

                  soup = BeautifulSoup(r.content, 'lxml')
                  table = soup.find('table', {'class': 'table'})
                  if not table:
                      print("  âŒ No table found"); return data, dates

                  headers_row = [h.get_text(strip=True).lower() for h in table.find_all('th')]
                  actual_idx = headers_row.index('actual') if 'actual' in headers_row else 1
                  date_idx   = headers_row.index('reference') if 'reference' in headers_row else None

                  for row in table.find_all('tr')[1:]:
                      cols = row.find_all('td')
                      if len(cols) < 2: continue
                      ctry = cols[0].get_text(strip=True)
                      for code, names in COUNTRY_NAMES.items():
                          if any(n.lower() in ctry.lower() for n in names):
                              val = clean_num(cols[actual_idx].get_text(strip=True))
                              dt  = parse_te_date(cols[date_idx].get_text(strip=True)) if date_idx and len(cols) > date_idx else str(date.today())
                              if val is not None:
                                  if label in ['consumerConfidence', 'businessConfidence']:
                                      val = normalize_confidence(val, code, label)
                                  data[code] = round(val, 4)
                                  dates[code] = dt
                                  print(f"  âœ“ {code}: {val} ({dt})")
                              break
              except Exception as e:
                  print(f"  âŒ {e}")
              return data, dates

          # â”€â”€ BOND YIELDS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          #
          # BUG CORREGIDO: La pÃ¡gina TE de bonds NO contiene <table class="table">.
          # Los yields aparecen en una plain <table> (widget de mercados) con columnas:
          #   Country | Actual | Chg | %Chg
          # La funciÃ³n anterior buscaba solo class="table" â†’ encontraba None â†’ retornaba vacÃ­o.
          #
          # Estrategia de 3 niveles:
          #   1. Intentar class="table" (por si TE lo cambia en el futuro)
          #   2. Escanear TODAS las tablas buscando (country, yield 0-20%)
          #   3. Fallback a pÃ¡ginas individuales por paÃ­s

          BOND_COUNTRY_MAP = {
              'USD': ['United States'],
              'EUR': ['Germany'],       # EUR proxy = Bund alemÃ¡n
              'GBP': ['United Kingdom'],
              'JPY': ['Japan'],
              'CAD': ['Canada'],
              'AUD': ['Australia'],
              'CHF': ['Switzerland'],
              'NZD': ['New Zealand'],
          }

          BOND_COUNTRY_SLUGS = {
              'USD': 'united-states',
              'EUR': 'germany',
              'GBP': 'united-kingdom',
              'JPY': 'japan',
              'CAD': 'canada',
              'AUD': 'australia',
              'CHF': 'switzerland',
              'NZD': 'new-zealand',
          }

          def _extract_bonds_from_all_tables(soup):
              """Escanea todas las tablas del HTML buscando yields vÃ¡lidos (0 < v < 20)."""
              data, dates = {}, {}
              for table in soup.find_all('table'):
                  for row in table.find_all('tr'):
                      cols = row.find_all('td')
                      if len(cols) < 2: continue
                      ctry_text = cols[0].get_text(strip=True)
                      for code, names in BOND_COUNTRY_MAP.items():
                          if code in data: continue
                          if any(n.lower() in ctry_text.lower() for n in names):
                              for idx in [1, 2, 3]:
                                  if idx >= len(cols): continue
                                  val = clean_num(cols[idx].get_text(strip=True))
                                  if val is not None and 0 < val < 20:
                                      data[code]  = round(val, 3)
                                      dates[code] = str(date.today())
                                      print(f"  âœ“ {code}: {val}% (widget table)")
                                      break
              return data, dates

          def _bond_country_page_fallback(code):
              """Fetcha la pÃ¡gina individual del paÃ­s como Ãºltimo recurso."""
              slug = BOND_COUNTRY_SLUGS.get(code)
              if not slug: return None, None
              url = f'https://tradingeconomics.com/{slug}/government-bond-yield'
              try:
                  r = requests.get(url, headers=HEADERS, timeout=15)
                  r.raise_for_status()
                  soup = BeautifulSoup(r.content, 'lxml')
                  for table in soup.find_all('table'):
                      for row in table.find_all('tr'):
                          for col in row.find_all('td'):
                              val = clean_num(col.get_text(strip=True))
                              if val is not None and 0 < val < 20:
                                  print(f"  âœ“ {code}: {val}% (country page fallback)")
                                  return round(val, 3), str(date.today())
              except Exception as e:
                  print(f"  âŒ Country page fallback {code}: {e}")
              return None, None

          def scrape_bonds():
              print(f"\n{'='*50}\nBOND YIELDS: TradingEconomics (3-level)\n{'='*50}")
              data, dates = {}, {}

              try:
                  r = requests.get(TE_URLS['bond10y'], headers=HEADERS, timeout=20)
                  if r.status_code == 429:
                      print("  âš ï¸ Rate limited, waiting 30s...")
                      time.sleep(30)
                      r = requests.get(TE_URLS['bond10y'], headers=HEADERS, timeout=20)
                  r.raise_for_status()
                  soup = BeautifulSoup(r.content, 'lxml')

                  # Nivel 1: class="table" (en caso de que TE lo aÃ±ada en el futuro)
                  std_table = soup.find('table', {'class': 'table'})
                  if std_table:
                      print("  Found class='table' â€” trying standard parser...")
                      headers_row = [h.get_text(strip=True).lower() for h in std_table.find_all('th')]
                      actual_idx = headers_row.index('actual') if 'actual' in headers_row else 1
                      date_idx   = headers_row.index('reference') if 'reference' in headers_row else None
                      for row in std_table.find_all('tr')[1:]:
                          cols = row.find_all('td')
                          if len(cols) < 2: continue
                          ctry = cols[0].get_text(strip=True)
                          for code, names in BOND_COUNTRY_MAP.items():
                              if any(n.lower() in ctry.lower() for n in names):
                                  val = clean_num(cols[actual_idx].get_text(strip=True))
                                  dt  = parse_te_date(cols[date_idx].get_text(strip=True)) if date_idx and len(cols) > date_idx else str(date.today())
                                  if val is not None and 0 < val < 20:
                                      data[code] = round(val, 3)
                                      dates[code] = dt
                                      print(f"  âœ“ {code}: {val}% (standard table)")
                                  break

                  # Nivel 2: Escanear todas las tablas (soluciona el bug actual)
                  missing = [c for c in CURRENCIES if c not in data]
                  if missing:
                      print(f"  Scanning all tables for: {', '.join(missing)}")
                      w_data, w_dates = _extract_bonds_from_all_tables(soup)
                      for code in missing:
                          if code in w_data:
                              data[code]  = w_data[code]
                              dates[code] = w_dates[code]

              except Exception as e:
                  print(f"  âŒ TE bond page error: {e}")

              # Nivel 3: Country-page fallback
              still_missing = [c for c in CURRENCIES if c not in data]
              if still_missing:
                  print(f"\n  Country-page fallback for: {', '.join(still_missing)}")
                  for code in still_missing:
                      val, dt = _bond_country_page_fallback(code)
                      if val is not None:
                          data[code]  = val
                          dates[code] = dt
                      time.sleep(1)

              # Resumen
              found    = [c for c in CURRENCIES if c in data]
              not_found = [c for c in CURRENCIES if c not in data]
              print(f"\n  Bonds found:   {found}")
              print(f"  Bonds missing: {not_found}")
              return data, dates

          # â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          all_data  = {c: {} for c in CURRENCIES}
          all_dates = {c: {} for c in CURRENCIES}

          # 1. Bond yields
          bond_data, bond_dates = scrape_bonds()
          for c in CURRENCIES:
              all_data[c]['bond10y']  = bond_data.get(c)
              all_dates[c]['bond10y'] = bond_dates.get(c, str(date.today()))
          time.sleep(2)

          # 2-6. Remaining indicators
          for key, url in {k: v for k, v in TE_URLS.items() if k != 'bond10y'}.items():
              d, dt = scrape_te_table(url, key)
              for c in CURRENCIES:
                  all_data[c][key]  = d.get(c)
                  all_dates[c][key] = dt.get(c, str(date.today()))
              time.sleep(2)

          # Save + validation summary
          print(f"\n{'='*50}\nSAVING RESULTS\n{'='*50}")
          for curr in CURRENCIES:
              pkg = {
                  'lastUpdate': str(date.today()),
                  'source': 'TradingEconomics â€” bonds+confidence fully normalized',
                  'data':   all_data[curr],
                  'dates':  all_dates[curr]
              }
              with open(f'extended-data/{curr}.json', 'w') as f:
                  json.dump(pkg, f, indent=2)
              available = [k for k, v in all_data[curr].items() if v is not None]
              missing   = [k for k in ['bond10y','consumerConfidence','businessConfidence',
                                        'capitalFlows','inflationExpectations'] if all_data[curr].get(k) is None]
              status = 'âœ…' if not missing else 'âš ï¸ '
              print(f"{status} {curr}: {', '.join(available) or 'NO DATA'}")
              if missing:
                  print(f"   Missing: {', '.join(missing)}")

          print("\nâœ… COMPLETE")
          EOFPYTHON

      - name: Commit and push
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add extended-data/
          if git diff --quiet && git diff --staged --quiet; then
            echo "No changes"
          else
            git commit -m "ğŸ“Š Extended data $(date +'%Y-%m-%d') â€” bonds+confidence fully fixed"
            git pull --rebase origin main || true
            git push
          fi
